{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import argparse\n",
    "import pickle\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from copy import deepcopy\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms\n",
    "import torch.utils.data as data_utils\n",
    "\n",
    "DATASET_DIR = \"data\"\n",
    "TEST_AT_EPOCH_END = True"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Args:\n",
    "    experiment = \"resnet\"  # choices = (\"mnist\", \"resnet\", \"vit\")\n",
    "    cuda = True\n",
    "    seed = 0\n",
    "    model_seed = 0\n",
    "    num_workers = 0\n",
    "    lr = 3e-3\n",
    "    batch_size = 32\n",
    "    epochs = 7\n",
    "\n",
    "args = Args()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utils"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LeNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# https://pytorch.org/tutorials/beginner/blitz/neural_networks_tutorial.html\n",
    "class LeNet5(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # 1 input image channel, 6 output channels, 5x5 square convolution\n",
    "        # kernel\n",
    "        self.conv1 = nn.Conv2d(1, 6, 5, padding=2)  # add padding\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        # an affine operation: y = Wx + b\n",
    "        self.fc1 = nn.Linear(16 * 5 * 5, 120)  # 5*5 from image dimension\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "        self.t = 1\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Max pooling over a (2, 2) window\n",
    "        x = F.max_pool2d(F.relu(self.conv1(x)), (2, 2))\n",
    "        # If the size is a square, you can specify with a single number\n",
    "        x = F.max_pool2d(F.relu(self.conv2(x)), 2)\n",
    "        x = torch.flatten(x, 1) # flatten all dimensions except the batch dimension\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ResNet9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_block(in_channels, out_channels, pool=False):\n",
    "    layers = [nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1), \n",
    "              nn.BatchNorm2d(out_channels), \n",
    "              nn.ReLU(inplace=True)]\n",
    "    if pool: layers.append(nn.MaxPool2d(2))\n",
    "    return nn.Sequential(*layers)\n",
    "\n",
    "class ResNet9(nn.Module):\n",
    "    def __init__(self, in_channels=3, num_classes=10):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.conv1 = conv_block(in_channels, 64)\n",
    "        self.conv2 = conv_block(64, 128, pool=True)\n",
    "        self.res1 = nn.Sequential(conv_block(128, 128), conv_block(128, 128))\n",
    "        \n",
    "        self.conv3 = conv_block(128, 256, pool=True)\n",
    "        self.conv4 = conv_block(256, 512, pool=True)\n",
    "        self.res2 = nn.Sequential(conv_block(512, 512), conv_block(512, 512))\n",
    "        \n",
    "        self.classifier = nn.Sequential(nn.MaxPool2d(4), \n",
    "                                        nn.Flatten(), \n",
    "                                        nn.Linear(512, num_classes))\n",
    "\n",
    "    def forward(self, xb):\n",
    "        out = self.conv1(xb)\n",
    "        out = self.conv2(out)\n",
    "        out = self.res1(out) + out\n",
    "        out = self.conv3(out)\n",
    "        out = self.conv4(out)\n",
    "        out = self.res2(out) + out\n",
    "        out = self.classifier(out)\n",
    "        return out\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ViT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/lucidrains/vit-pytorch/blob/main/vit_pytorch/simple_vit.py\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "from einops import rearrange\n",
    "from einops.layers.torch import Rearrange\n",
    "\n",
    "from math import log2\n",
    "\n",
    "# helpers\n",
    "\n",
    "def pair(t):\n",
    "    return t if isinstance(t, tuple) else (t, t)\n",
    "\n",
    "def posemb_sincos_2d(patches, temperature = 10000, dtype = torch.float32):\n",
    "    _, h, w, dim, device, dtype = *patches.shape, patches.device, patches.dtype\n",
    "\n",
    "    y, x = torch.meshgrid(torch.arange(h, device = device), torch.arange(w, device = device), indexing = 'ij')\n",
    "    assert (dim % 4) == 0, 'feature dimension must be multiple of 4 for sincos emb'\n",
    "    omega = torch.arange(dim // 4, device = device) / (dim // 4 - 1)\n",
    "    omega = 1. / (temperature ** omega)\n",
    "\n",
    "    y = y.flatten()[:, None] * omega[None, :]\n",
    "    x = x.flatten()[:, None] * omega[None, :] \n",
    "    pe = torch.cat((x.sin(), x.cos(), y.sin(), y.cos()), dim = 1)\n",
    "    return pe.type(dtype)\n",
    "\n",
    "# classes\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, dim, hidden_dim):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.LayerNorm(dim),\n",
    "            nn.Linear(dim, hidden_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(hidden_dim, dim),\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, dim, heads = 8, dim_head = 64):\n",
    "        super().__init__()\n",
    "        inner_dim = dim_head *  heads\n",
    "        self.heads = heads\n",
    "        self.scale = dim_head ** -0.5\n",
    "        self.norm = nn.LayerNorm(dim)\n",
    "\n",
    "        self.attend = nn.Softmax(dim = -1)\n",
    "\n",
    "        self.to_qkv = nn.Linear(dim, inner_dim * 3, bias = False)\n",
    "        self.to_out = nn.Linear(inner_dim, dim, bias = False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.norm(x)\n",
    "        magic_const = (self.heads / 2 * (1 + log2(self.heads))) ** (1 / 3)\n",
    "        qkv = self.to_qkv(x).div(magic_const).chunk(3, dim = -1)\n",
    "        q, k, v = map(lambda t: rearrange(t, 'b n (h d) -> b h n d', h = self.heads), qkv)\n",
    "\n",
    "        dots = torch.matmul(q, k.transpose(-1, -2)) * self.scale\n",
    "\n",
    "        attn = self.attend(dots)\n",
    "\n",
    "        out = torch.matmul(attn, v)\n",
    "        out = rearrange(out, 'b h n d -> b n (h d)')\n",
    "        return self.to_out(out)\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, dim, depth, heads, dim_head, mlp_dim):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList([])\n",
    "        for _ in range(depth):\n",
    "            self.layers.append(nn.ModuleList([\n",
    "                Attention(dim, heads = heads, dim_head = dim_head),\n",
    "                FeedForward(dim, mlp_dim)\n",
    "            ]))\n",
    "    def forward(self, x):\n",
    "        for attn, ff in self.layers:\n",
    "            x = attn(x) + x\n",
    "            x = ff(x) + x\n",
    "        return x\n",
    "\n",
    "class SimpleViT(nn.Module):\n",
    "    def __init__(self, *, image_size, patch_size, num_classes, dim, depth, heads, mlp_dim, channels = 3, dim_head = 64):\n",
    "        super().__init__()\n",
    "        image_height, image_width = pair(image_size)\n",
    "        patch_height, patch_width = pair(patch_size)\n",
    "\n",
    "        assert image_height % patch_height == 0 and image_width % patch_width == 0, 'Image dimensions must be divisible by the patch size.'\n",
    "\n",
    "        num_patches = (image_height // patch_height) * (image_width // patch_width)\n",
    "        patch_dim = channels * patch_height * patch_width\n",
    "\n",
    "        self.to_patch_embedding = nn.Sequential(\n",
    "            Rearrange('b c (h p1) (w p2) -> b h w (p1 p2 c)', p1 = patch_height, p2 = patch_width),\n",
    "            nn.LayerNorm(patch_dim),\n",
    "            nn.Linear(patch_dim, dim),\n",
    "            nn.LayerNorm(dim),\n",
    "        )\n",
    "\n",
    "        self.transformer = Transformer(dim, depth, heads, dim_head, mlp_dim)\n",
    "\n",
    "        self.to_latent = nn.Identity()\n",
    "        self.linear_head = nn.Sequential(\n",
    "            nn.LayerNorm(dim),\n",
    "            nn.Linear(dim, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, img):\n",
    "        *_, h, w, dtype = *img.shape, img.dtype\n",
    "\n",
    "        x = self.to_patch_embedding(img)\n",
    "        pe = posemb_sincos_2d(x)\n",
    "        x = rearrange(x, 'b ... d -> b (...) d') + pe\n",
    "\n",
    "        x = self.transformer(x)\n",
    "        x = x.mean(dim = 1)\n",
    "\n",
    "        x = self.to_latent(x)\n",
    "        return self.linear_head(x)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Params Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def get_flat_params(optimizer):\n",
    "    return torch.cat([p.view(-1) for group in optimizer.param_groups for p in group['params']])\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def get_flat_grads(optimizer):\n",
    "    return torch.cat([p.grad.view(-1) for group in optimizer.param_groups for p in group['params'] if p.grad is not None])\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def get_flat_exp_avg_sq(optimizer):\n",
    "    return torch.cat([optimizer.state[p][\"exp_avg_sq\"].view(-1) for group in optimizer.param_groups for p in group['params'] if \"exp_avg_sq\" in optimizer.state[p]])\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def set_flat_params(optimizer, flat_params):\n",
    "    j = 0\n",
    "    for group in optimizer.param_groups:\n",
    "        for p in group['params']:\n",
    "            p.data.copy_(flat_params[j:j + p.numel()].reshape_as(p).data)\n",
    "            j += p.numel()\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def set_flat_grads(optimizer, flat_grads):\n",
    "    j = 0\n",
    "    for group in optimizer.param_groups:\n",
    "        for p in group['params']:\n",
    "            if p.grad is not None:\n",
    "                p.grad.data.copy_(flat_grads[j:j + p.numel()].reshape_as(p).data)\n",
    "                j += p.numel()\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def set_model_flat_params(model, flat_params):\n",
    "    j = 0\n",
    "    for p in model.parameters():\n",
    "        p.data.copy_(flat_params[j:j + p.numel()].reshape_as(p).data)\n",
    "        j += p.numel()\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def check_sparsity(model, eps=1e-8, show_results=True):\n",
    "    nonzeros = sum((p.abs() > eps).sum().item() for p in model.parameters())\n",
    "    total_params = sum((p.numel() for p in model.parameters()))\n",
    "    sparsity = nonzeros / total_params\n",
    "    if show_results:\n",
    "        print(f\"Sparsity = {100 * sparsity:.2f}%\")\n",
    "    return sparsity"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def test(model, loss_fn, test_loader, device, show_results=True, get_gradnorm=False):\n",
    "    model.eval()\n",
    "\n",
    "    def closure():\n",
    "        test_loss = 0\n",
    "        correct = 0\n",
    "        for x, y in test_loader:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            y_pred = model(x)\n",
    "            loss = loss_fn(y_pred, y) / len(test_loader)\n",
    "            if get_gradnorm:\n",
    "                loss.backward()\n",
    "            test_loss += loss.item()\n",
    "            # Accuracy\n",
    "            pred = y_pred.max(dim=1).indices\n",
    "            correct += (pred == y).sum().item()\n",
    "        return test_loss, correct\n",
    "\n",
    "    if get_gradnorm:\n",
    "        closure = torch.enable_grad()(closure)\n",
    "        test_loss, correct = closure()\n",
    "        flat_grad = []\n",
    "        for i, p in enumerate(model.parameters()):\n",
    "            if p.grad is not None:\n",
    "                flat_grad.append(p.grad.view(-1))\n",
    "                p.grad = None\n",
    "        gradnorm = torch.cat(flat_grad).pow(2).sum().sqrt().item()\n",
    "    else:\n",
    "        test_loss, correct = closure()\n",
    "        gradnorm = 0.0\n",
    "\n",
    "    sparsity = check_sparsity(model, show_results=False)\n",
    "    acc = correct / len(test_loader.dataset)\n",
    "    error = 1 - acc\n",
    "    if show_results:\n",
    "        print(f\"\\nTest set: Average loss: {test_loss:.4f}, \"\n",
    "              f\"Accuracy: {correct}/{len(test_loader.dataset)} ({100. * acc:.0f}%), \"\n",
    "              f\"Sparsity: {100. * sparsity:.2f}%\\n\")\n",
    "\n",
    "    return test_loss, gradnorm, error, sparsity\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, optimizer, loss_fn, train_loader, test_loader, device, epoch,\n",
    "          log_interval=50, test_interval=0.25, get_gradnorm=False):\n",
    "    model.train()\n",
    "    data = []\n",
    "    # Add test at initial point\n",
    "    if epoch == 1:\n",
    "        result0 = test(model, loss_fn, test_loader, device, show_results=True, get_gradnorm=get_gradnorm)\n",
    "        data.append((0.,) + result0)\n",
    "\n",
    "    # Training loop\n",
    "    for batch_idx, (x, y) in enumerate(train_loader):\n",
    "        x, y = x.to(device), y.to(device)\n",
    "\n",
    "        def closure():\n",
    "            optimizer.zero_grad()\n",
    "            y_pred = model(x)\n",
    "            loss = loss_fn(y_pred, y)\n",
    "            loss.backward()\n",
    "            return loss\n",
    "\n",
    "        loss = optimizer.step(closure)\n",
    "        # Logging\n",
    "        if batch_idx % log_interval == 0:\n",
    "            batch_ratio = batch_idx / len(train_loader)\n",
    "            sparsity = check_sparsity(model, show_results=False)\n",
    "            print(f\"Train Epoch: {epoch} [{batch_idx * len(x)}/{len(train_loader.dataset)} \"\n",
    "                  f\"({100. * batch_ratio:.0f}%)]\\tLoss: {loss.item():.6f},\\tSparsity: {100. * sparsity:.2f}%\")\n",
    "        # Testing\n",
    "        should_test = (batch_idx + 1) % round(test_interval * len(train_loader)) == 0\n",
    "        last_epoch = batch_idx == len(train_loader) - 1\n",
    "        if should_test or last_epoch:\n",
    "            ep = epoch - 1 + (batch_idx + 1) / len(train_loader)\n",
    "            # Show results if last batch\n",
    "            result = test(model, loss_fn, test_loader, device, show_results=last_epoch, get_gradnorm=get_gradnorm)\n",
    "            data.append((ep,) + result)\n",
    "\n",
    "    return data\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizer Wrappers"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DecoupledDecay Wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoupledDecay(torch.optim.Optimizer):\n",
    "    def __init__(self, params, decay_type=None, decay_rate=1e-3, decay_threshold=2e-4, **kwargs):\n",
    "        super().__init__(params, **kwargs)\n",
    "        self.decay_type = decay_type\n",
    "        self.decay_rate = decay_rate\n",
    "        self.decay_threshold = decay_threshold\n",
    "        self.decay = {}\n",
    "\n",
    "    def __repr__(self):\n",
    "        return \"Decayed.\" + super().__repr__()\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def step(self, closure):\n",
    "        if self.decay_type is None:\n",
    "            return super().step(closure)\n",
    " \n",
    "        # calculate decay\n",
    "        for group in self.param_groups:\n",
    "            for p in group['params']:\n",
    "                if self.decay_type == \"l1\":\n",
    "                    decay = torch.sign(p).clone()\n",
    "                elif self.decay_type == \"hinge\":\n",
    "                    if \"exp_avg_sq\" in self.state[p]:\n",
    "                        scale = self.state[p][\"exp_avg_sq\"].add(1e-12).pow(-0.5)\n",
    "                    else:\n",
    "                        scale = torch.ones_like(p)\n",
    "                    if self.decay_threshold >= 0:\n",
    "                        decay_region = p.abs() > self.decay_threshold * scale\n",
    "                    else:\n",
    "                        decay_region = p.abs() < -self.decay_threshold * scale\n",
    "                    decay = torch.zeros_like(p)\n",
    "                    decay[decay_region] = torch.sign(p[decay_region]).clone()\n",
    "                else:\n",
    "                    decay = p.clone()\n",
    "                self.decay[p] = group['lr'] * self.decay_rate * decay\n",
    "        # step\n",
    "        loss = super().step(closure)\n",
    "        # apply decoupled decay\n",
    "        for group in self.param_groups:\n",
    "            for p in group['params']:\n",
    "                p.sub_(self.decay[p])\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Masked Optimizer Wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaskedOptimizer(torch.optim.Optimizer):\n",
    "    def __init__(self, *args, prune_threshold=1e-8, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.mask = {}\n",
    "        self.prune_threshold = prune_threshold\n",
    "        self.use_masked_closure = False  # TODO\n",
    "\n",
    "    def __repr__(self):\n",
    "        return \"Masked.\" + super().__repr__()\n",
    "\n",
    "    def _parameters(self):\n",
    "        for group in self.param_groups:\n",
    "            for param in group['params']:\n",
    "                yield param\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def update_mask(self):\n",
    "        for p in self._parameters():\n",
    "            if p in self.mask:\n",
    "                self.mask[p] &= p != 0.\n",
    "            else:\n",
    "                self.mask[p] = p != 0.\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def mask_params(self):\n",
    "        for p in self._parameters():\n",
    "            if p in self.mask:\n",
    "                p.mul_(self.mask[p])\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def mask_grads(self):\n",
    "        for p in self._parameters():\n",
    "            if p in self.mask[p] and p.grad is not None:\n",
    "                p.grad.mul_(self.mask[p])\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def prune_smallest(self, sparsity=0.9):\n",
    "        flat_params = get_flat_params(self)\n",
    "        sparse_elems = max(1, round((1 - sparsity) * flat_params.numel()))\n",
    "        threshold = torch.topk(flat_params.abs().cpu(), k=sparse_elems).values[-1]\n",
    "        flat_params[flat_params.abs() < threshold] = 0.\n",
    "        set_flat_params(self, flat_params)\n",
    "        self.update_mask()\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def prune_smaller_than(self, threshold=None):\n",
    "        if threshold is None:\n",
    "            # threshold = self.prune_threshold  # XXX\n",
    "            threshold = self.prune_threshold * (get_flat_exp_avg_sq(self).sqrt() + 1e-8)\n",
    "        flat_params = get_flat_params(self)\n",
    "        flat_params[flat_params.abs() < threshold] = 0.\n",
    "        set_flat_params(self, flat_params)\n",
    "        self.update_mask()\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def masked_closure(self, closure):\n",
    "        closure = torch.enable_grad()(closure)\n",
    "\n",
    "        def closure_then_mask_grads():\n",
    "            closure()\n",
    "            self.mask_grads()\n",
    "\n",
    "        return closure_then_mask_grads\n",
    "\n",
    "    def step(self, closure):\n",
    "        if self.use_masked_closure:\n",
    "            closure = self.masked_closure(closure)\n",
    "        loss = super().step(closure)  # assuming first-order step\n",
    "        self.prune_smaller_than()\n",
    "        self.mask_params()\n",
    "        return loss\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def prune_smallest(optimizer, sparsity=0.9):\n",
    "    flat_params = get_flat_params(optimizer)\n",
    "    sparse_elems = max(1, round((1 - sparsity) * flat_params.numel()))\n",
    "    threshold = torch.topk(flat_params.abs().cpu(), k=sparse_elems).values[-1]\n",
    "    flat_params[flat_params.abs() < threshold] = 0.\n",
    "    set_flat_params(optimizer, flat_params)\n",
    "    optimizer.update_mask()\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def g2_prune_smaller_than(optimizer, threshold):\n",
    "    flat_params = get_flat_params(optimizer)\n",
    "    flat_exp_avg_sq = torch.cat([optimizer.state[p][\"exp_avg_sq\"].view(-1)\n",
    "                                 for group in optimizer.param_groups for p in group['params'] if \"exp_avg_sq\" in optimizer.state[p]])\n",
    "    flat_params[flat_exp_avg_sq.abs() < threshold] = 0.\n",
    "    set_flat_params(optimizer, flat_params)\n",
    "    return flat_params\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def g2_prune_bigger_than(optimizer, threshold):\n",
    "    flat_params = get_flat_params(optimizer)\n",
    "    flat_exp_avg_sq = torch.cat([optimizer.state[p][\"exp_avg_sq\"].view(-1)\n",
    "                                 for group in optimizer.param_groups for p in group['params'] if \"exp_avg_sq\" in optimizer.state[p]])\n",
    "    flat_params[flat_exp_avg_sq.abs() > threshold] = 0.\n",
    "    set_flat_params(optimizer, flat_params)\n",
    "    return flat_params"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SAM Wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SAMBase(torch.optim.Optimizer):\n",
    "    def __init__(self, params, rho=0.05, scaled_max='gradnorm', **kwargs):\n",
    "        assert rho >= 0.0, f\"Invalid rho, should be non-negative: {rho}\"\n",
    "        self.adam_initialized = False\n",
    "        super().__init__(params, **kwargs)\n",
    "        for group in self.param_groups:\n",
    "            group.setdefault(\"rho\", rho)\n",
    "            group.setdefault(\"scaled_max\", scaled_max)\n",
    "\n",
    "    def _init_adam(self, closure):\n",
    "        for group in self.param_groups:\n",
    "            for p in group[\"params\"]:\n",
    "                p.grad = torch.zeros_like(p)\n",
    "        super().step(closure)  # dummy step\n",
    "        self.adam_initialized = True\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def step(self, closure=None):\n",
    "        assert closure is not None, \"Sharpness Aware Minimization requires closure, but it was not provided\"\n",
    "        if not self.adam_initialized:\n",
    "            self._init_adam(closure)\n",
    "\n",
    "        closure = torch.enable_grad()(closure)\n",
    "\n",
    "        loss = closure()\n",
    "        grad_norm = self._grad_norm()\n",
    "        for group in self.param_groups:\n",
    "            for p in group[\"params\"]:\n",
    "                if p.grad is None:\n",
    "                    continue\n",
    "                # how to scale max step\n",
    "                if group[\"scaled_max\"] == \"none\":\n",
    "                    scale = torch.ones(1)\n",
    "                elif group[\"scaled_max\"] == \"adam\" and \"exp_avg_sq\" in self.state[p]:\n",
    "                    scale = self.state[p][\"exp_avg_sq\"].add(1e-12).pow(-0.5)\n",
    "                else:\n",
    "                    scale = (grad_norm + 1e-12).pow(-1)\n",
    "                # step\n",
    "                self.state[p][\"old_p\"] = p.data.clone()\n",
    "                g = p.grad if \"exp_avg\" not in self.state[p] else self.state[p][\"exp_avg\"]\n",
    "                p.add_(scale.to(p) * g, alpha=group[\"lr\"] * group[\"rho\"])\n",
    "                # if isinstance(self, DecoupledDecay):\n",
    "                #     p.add_(torch.sign(p).clone(), alpha=group[\"lr\"] * group[\"rho\"] * self.decay_rate)  # XXX\n",
    "                p.grad = None\n",
    "\n",
    "        closure()  # get grad at w+e\n",
    "        for group in self.param_groups:\n",
    "            for p in group[\"params\"]:\n",
    "                if p.grad is None:\n",
    "                    continue\n",
    "                p.data.copy_(self.state[p][\"old_p\"])  # set params back to w\n",
    "\n",
    "        super().step()  # \"sharpness-aware\" update\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def _grad_norm(self):\n",
    "        # put everything on the same device, in case of model parallelism\n",
    "        shared_device = self.param_groups[0][\"params\"][0].device\n",
    "        gradnorm = torch.stack([\n",
    "            p.grad.pow(2).sum().to(shared_device)\n",
    "            for group in self.param_groups for p in group[\"params\"] if p.grad is not None\n",
    "        ]).sum().sqrt()\n",
    "        return gradnorm\n",
    "\n",
    "    def __repr__(self):\n",
    "        return \"SharpnessAware.\" + super().__repr__()\n",
    "\n",
    "\n",
    "class SAM(SAMBase, torch.optim.SGD):\n",
    "    pass\n",
    "\n",
    "class SADAM(SAMBase, torch.optim.Adam):\n",
    "    pass"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_masked_and_decayed(Optimizer):\n",
    "    class MaskedDecayedOptimizer(MaskedOptimizer, DecoupledDecay, Optimizer):\n",
    "        def __repr__(self):\n",
    "            return Optimizer.__name__ + \".\" + super().__repr__()\n",
    "        pass\n",
    "    return MaskedDecayedOptimizer\n",
    "\n",
    "\n",
    "def init_device(cuda):\n",
    "    use_cuda = torch.cuda.is_available() and cuda\n",
    "    use_mps = torch.backends.mps.is_available() and cuda\n",
    "    device = torch.device(\"cuda\" if use_cuda else (\"mps\" if use_mps else \"cpu\"))\n",
    "    if use_cuda:\n",
    "        print(f\"Using CUDA\")\n",
    "    if use_mps:\n",
    "        print(f\"Using MPS\")\n",
    "\n",
    "\n",
    "def init_seed(seed):\n",
    "    if seed is not None:\n",
    "        print(f\"Setting random seed to {seed}.\")\n",
    "        np.random.seed(seed)\n",
    "        torch.manual_seed(seed)\n",
    "\n",
    "\n",
    "def init_model(experiment=\"lenet\"):\n",
    "    experiment = experiment.lower()\n",
    "    if experiment == \"lenet\":\n",
    "        model = LeNet5()\n",
    "\n",
    "    elif experiment == \"resnet\":\n",
    "        model = ResNet9()\n",
    "\n",
    "    elif experiment == \"vit\":\n",
    "        model = SimpleViT(image_size=32,\n",
    "                    patch_size=8,\n",
    "                    num_classes=10,  # for cifar 10\n",
    "                    dim=128,\n",
    "                    depth=12,\n",
    "                    heads=8,\n",
    "                    mlp_dim=64)\n",
    "    \n",
    "    else:\n",
    "        raise NotImplementedError(f\"experiment '{experiment}' not implemented.\")\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "@ torch.no_grad()\n",
    "def count_params(model):\n",
    "    return sum(p.numel() for p in model.parameters())\n",
    "\n",
    "\n",
    "def load_dataset(experiment=\"lenet\"):\n",
    "    experiment = experiment.lower()\n",
    "    if experiment == \"lenet\":\n",
    "        transform = transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.5,), (0.5,))\n",
    "        ])\n",
    "        train_dataset = datasets.MNIST(DATASET_DIR, train=True, download=True, transform=transform)\n",
    "        test_dataset = datasets.MNIST(DATASET_DIR, train=False, transform=transform)\n",
    "\n",
    "    elif experiment in (\"resnet\", \"vit\"):\n",
    "        image_size = 32\n",
    "        transform = transforms.Compose([\n",
    "            transforms.Grayscale(3),\n",
    "            transforms.Resize(image_size),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.5,), (0.5,)),\n",
    "        ])\n",
    "        train_dataset = datasets.MNIST(DATASET_DIR, train=True, download=True, transform=transform)\n",
    "        test_dataset = datasets.MNIST(DATASET_DIR, train=False, transform=transform)\n",
    "        # cifar_mean = (0.4914, 0.4822, 0.4465)\n",
    "        # cifar_std = (0.2023, 0.1994, 0.2010)\n",
    "        # train_transform = transforms.Compose([\n",
    "        #     transforms.RandomCrop(image_size, padding=4),\n",
    "        #     transforms.RandomHorizontalFlip(),\n",
    "        #     transforms.ToTensor(),\n",
    "        #     transforms.Normalize(cifar_mean, cifar_std)\n",
    "        # ])\n",
    "        # test_transform = transforms.Compose([\n",
    "        #     transforms.ToTensor(),\n",
    "        #     transforms.Normalize(cifar10_mean, cifar10_std)\n",
    "        # ])\n",
    "        # train_dataset = datasets.CIFAR10(DATASET_DIR, train=True, download=True, transform=train_transform)\n",
    "        # test_dataset = datasets.CIFAR10(DATASET_DIR, train=False, transform=test_transform)\n",
    "\n",
    "    else:\n",
    "        raise NotImplementedError(f\"experiment '{experiment}' not implemented.\")\n",
    "\n",
    "\n",
    "    return train_dataset, test_dataset\n",
    "\n",
    "\n",
    "def init_experiment(args, Optimizer=torch.optim.SGD, **optim_args):\n",
    "    device = init_device(args.cuda)\n",
    "    \n",
    "    # All runs start are init based on the model seed\n",
    "    print(f\"Initializing model with random seed {args.model_seed}.\")\n",
    "    init_seed(args.model_seed)\n",
    "    model = init_model(args.experiment).to(device)\n",
    "\n",
    "    # Load dataset and initialize dataloader\n",
    "    init_seed(args.seed)\n",
    "    train_dataset, test_dataset = load_dataset(args.experiment)\n",
    "    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=args.batch_size,\n",
    "                                                shuffle=True, num_workers=args.num_workers)\n",
    "    test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=args.batch_size,\n",
    "                                                shuffle=False, num_workers=args.num_workers)\n",
    "\n",
    "    # Loss and optimizer\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    MaskedDecayedOptimizer = make_masked_and_decayed(Optimizer)\n",
    "    optimizer = MaskedDecayedOptimizer(model.parameters(), **optim_args)\n",
    "\n",
    "    print(f\"Model has {count_params(model)} parameters\")\n",
    "\n",
    "    return model, optimizer, loss_fn, train_loader, test_loader, device\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MPS\n",
      "Initializing model with random seed 0.\n",
      "Setting random seed to 0.\n",
      "Setting random seed to 0.\n",
      "Model has 6575370 parameters\n",
      "Using MPS\n",
      "Initializing model with random seed 0.\n",
      "Setting random seed to 0.\n",
      "Setting random seed to 0.\n",
      "Model has 6575370 parameters\n",
      "Using MPS\n",
      "Initializing model with random seed 0.\n",
      "Setting random seed to 0.\n",
      "Setting random seed to 0.\n",
      "Model has 6575370 parameters\n",
      "Using MPS\n",
      "Initializing model with random seed 0.\n",
      "Setting random seed to 0.\n",
      "Setting random seed to 0.\n",
      "Model has 6575370 parameters\n",
      "Using MPS\n",
      "Initializing model with random seed 0.\n",
      "Setting random seed to 0.\n",
      "Setting random seed to 0.\n",
      "Model has 6575370 parameters\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(ResNet9(\n",
       "   (conv1): Sequential(\n",
       "     (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "     (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "     (2): ReLU(inplace=True)\n",
       "   )\n",
       "   (conv2): Sequential(\n",
       "     (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "     (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "     (2): ReLU(inplace=True)\n",
       "     (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "   )\n",
       "   (res1): Sequential(\n",
       "     (0): Sequential(\n",
       "       (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "       (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "       (2): ReLU(inplace=True)\n",
       "     )\n",
       "     (1): Sequential(\n",
       "       (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "       (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "       (2): ReLU(inplace=True)\n",
       "     )\n",
       "   )\n",
       "   (conv3): Sequential(\n",
       "     (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "     (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "     (2): ReLU(inplace=True)\n",
       "     (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "   )\n",
       "   (conv4): Sequential(\n",
       "     (0): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "     (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "     (2): ReLU(inplace=True)\n",
       "     (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "   )\n",
       "   (res2): Sequential(\n",
       "     (0): Sequential(\n",
       "       (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "       (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "       (2): ReLU(inplace=True)\n",
       "     )\n",
       "     (1): Sequential(\n",
       "       (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "       (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "       (2): ReLU(inplace=True)\n",
       "     )\n",
       "   )\n",
       "   (classifier): Sequential(\n",
       "     (0): MaxPool2d(kernel_size=4, stride=4, padding=0, dilation=1, ceil_mode=False)\n",
       "     (1): Flatten(start_dim=1, end_dim=-1)\n",
       "     (2): Linear(in_features=512, out_features=10, bias=True)\n",
       "   )\n",
       " ),\n",
       " SADAM.Masked.Decayed.SharpnessAware.MaskedDecayedOptimizer (\n",
       " Parameter Group 0\n",
       "     amsgrad: False\n",
       "     betas: (0.9, 0.999)\n",
       "     capturable: False\n",
       "     differentiable: False\n",
       "     eps: 1e-08\n",
       "     foreach: None\n",
       "     fused: None\n",
       "     lr: 0.003\n",
       "     maximize: False\n",
       "     rho: 0.0015\n",
       "     scaled_max: gradnorm\n",
       "     weight_decay: 0\n",
       " ),\n",
       " CrossEntropyLoss(),\n",
       " <torch.utils.data.dataloader.DataLoader at 0x7fd5f6af16f0>,\n",
       " <torch.utils.data.dataloader.DataLoader at 0x7fd5f6af1e70>,\n",
       " None)"
      ]
     },
     "execution_count": 324,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# testing inits\n",
    "args = Args()\n",
    "init_experiment(args, Optimizer=torch.optim.SGD, lr=args.lr)\n",
    "init_experiment(args, Optimizer=torch.optim.SGD, lr=args.lr, momentum=0.9, decay_type=\"l2\")\n",
    "init_experiment(args, Optimizer=torch.optim.Adam, lr=args.lr, betas=(0.9, 0.999))\n",
    "init_experiment(args, Optimizer=SAM, lr=args.lr, momentum=0.9, rho=args.lr/2)\n",
    "init_experiment(args, Optimizer=SADAM, lr=args.lr, betas=(0.9, 0.999), rho=args.lr/2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiments 1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Does Sharpness Aware Minimization help find sparser solutions and better lottery tickets?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train\n",
    "model, optimizer, loss_fn, train_loader, test_loader, device = init_experiment(\n",
    "    args, Optimizer=torch.optim.SGD, lr=args.lr, momentum=0.9, decay_type=\"l1\")\n",
    "\n",
    "data = []\n",
    "for epoch in range(1, args.epochs + 1):\n",
    "    results = train(model, optimizer, loss_fn, train_loader, test_loader, device, epoch)\n",
    "    data += results\n",
    "    # prune\n",
    "    check_sparsity(model)\n",
    "    optimizer.prune_smallest(sparsity=1 - 0.5**epoch)\n",
    "    check_sparsity(model)\n",
    "    test(model, loss_fn, test_loader, device)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train\n",
    "model, optimizer, loss_fn, train_loader, test_loader, device = init_experiment(\n",
    "    args, Optimizer=SAM, lr=args.lr, momentum=0.9, rho=args.lr/2, decay_type=\"l1\")\n",
    "\n",
    "data = []\n",
    "for epoch in range(1, args.epochs + 1):\n",
    "    results = train(model, optimizer, loss_fn, train_loader, test_loader, device, epoch)\n",
    "    data += results\n",
    "    # prune\n",
    "    check_sparsity(model)\n",
    "    optimizer.prune_smallest(sparsity=1 - 0.5**epoch)\n",
    "    check_sparsity(model)\n",
    "    test(model, loss_fn, test_loader, device)\n",
    "    print()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test AdamW vs. DecayedAdam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train\n",
    "model, optimizer, loss_fn, train_loader, test_loader, device = init_experiment(\n",
    "    args, Optimizer=torch.optim.Adam, lr=args.lr, decay_type='l2', decay_rate=1e-2)\n",
    "data = []\n",
    "for epoch in range(1, args.epochs + 1):\n",
    "    results = train(model, optimizer, loss_fn, train_loader, test_loader, device, epoch)\n",
    "    data += results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train\n",
    "model, optimizer, loss_fn, train_loader, test_loader, device = init_experiment(\n",
    "    args, Optimizer=torch.optim.AdamW, lr=args.lr, weight_decay=1e-2)\n",
    "data = []\n",
    "for epoch in range(1, args.epochs + 1):\n",
    "    results = train(model, optimizer, loss_fn, train_loader, test_loader, device, epoch)\n",
    "    data += results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train\n",
    "model, optimizer, loss_fn, train_loader, test_loader, device = init_experiment(\n",
    "    args, Optimizer=torch.optim.Adam, lr=args.lr)\n",
    "data = []\n",
    "for epoch in range(1, args.epochs + 1):\n",
    "    results = train(model, optimizer, loss_fn, train_loader, test_loader, device, epoch)\n",
    "    data += results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train\n",
    "model, optimizer, loss_fn, train_loader, test_loader, device = init_experiment(\n",
    "    args, Optimizer=torch.optim.Adam, lr=args.lr, decay_type='l1', decay_rate=1e-2)\n",
    "data = []\n",
    "for epoch in range(1, args.epochs + 1):\n",
    "    results = train(model, optimizer, loss_fn, train_loader, test_loader, device, epoch)\n",
    "    data += results\n",
    "    check_sparsity(model)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sharpness-Aware Adam with decoupled L1 decay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train\n",
    "args.lr = 1e-2\n",
    "model, optimizer, loss_fn, train_loader, test_loader, device = init_experiment(\n",
    "    args, Optimizer=SADAM, lr=args.lr, rho=0.005, decay_type='l1', decay_rate=1e-3, scaled_max=\"adam\")\n",
    "sched = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.9)\n",
    "data = []\n",
    "for epoch in range(1, args.epochs + 1):\n",
    "    results = train(model, optimizer, loss_fn, train_loader, test_loader, device, epoch)\n",
    "    sched.step()\n",
    "    data += results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(1, args.epochs + 1):\n",
    "    results = train(model, optimizer, loss_fn, train_loader, test_loader, device, epoch)\n",
    "    sched.step()\n",
    "    data += results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(1, args.epochs + 1):\n",
    "    results = train(model, optimizer, loss_fn, train_loader, test_loader, device, epoch)\n",
    "    sched.step()\n",
    "    data += results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(1, args.epochs + 1):\n",
    "    results = train(model, optimizer, loss_fn, train_loader, test_loader, device, epoch)\n",
    "    sched.step()\n",
    "    data += results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(1, args.epochs + 1):\n",
    "    results = train(model, optimizer, loss_fn, train_loader, test_loader, device, epoch)\n",
    "    sched.step()\n",
    "    data += results"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiments 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MPS\n",
      "Initializing model with random seed 0.\n",
      "Setting random seed to 0.\n",
      "Setting random seed to 0.\n",
      "Model has 6575370 parameters\n"
     ]
    }
   ],
   "source": [
    "rho = 0.01  # relative to lr\n",
    "decay_rate = 0.01  # relative to lr\n",
    "prune_threshold = 0.1  # relative to exp_avg_sq\n",
    "decay_type = 'l1'\n",
    "decay_threshold = 1. * prune_threshold  # XXX\n",
    "optim_args = dict(lr=args.lr, rho=rho, scaled_max=\"adam\",\n",
    "                  decay_type=decay_type, decay_rate=decay_rate, decay_threshold=decay_threshold, prune_threshold=prune_threshold)\n",
    "\n",
    "model, optimizer, loss_fn, train_loader, test_loader, device = init_experiment(args, Optimizer=SADAM, **optim_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_state_dict0 = deepcopy(model.state_dict())\n",
    "optimizer_state_dict0 = deepcopy(optimizer.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "model.load_state_dict(model_state_dict0)\n",
    "optimizer.load_state_dict(optimizer_state_dict0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 2.3021, Accuracy: 1135/10000 (11%), Sparsity: 99.97%\n",
      "\n",
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 19.540150,\tSparsity: 99.77%\n",
      "Train Epoch: 1 [1600/60000 (3%)]\tLoss: 2.310075,\tSparsity: 95.62%\n",
      "Train Epoch: 1 [3200/60000 (5%)]\tLoss: 2.298961,\tSparsity: 94.45%\n",
      "Train Epoch: 1 [4800/60000 (8%)]\tLoss: 2.294398,\tSparsity: 93.57%\n",
      "Train Epoch: 1 [6400/60000 (11%)]\tLoss: 2.287789,\tSparsity: 92.59%\n",
      "Train Epoch: 1 [8000/60000 (13%)]\tLoss: 2.317114,\tSparsity: 91.50%\n",
      "Train Epoch: 1 [9600/60000 (16%)]\tLoss: 2.306354,\tSparsity: 90.41%\n",
      "Train Epoch: 1 [11200/60000 (19%)]\tLoss: 2.312703,\tSparsity: 89.34%\n",
      "Train Epoch: 1 [12800/60000 (21%)]\tLoss: 2.297533,\tSparsity: 88.21%\n",
      "Train Epoch: 1 [14400/60000 (24%)]\tLoss: 2.290428,\tSparsity: 87.07%\n",
      "Train Epoch: 1 [16000/60000 (27%)]\tLoss: 2.309808,\tSparsity: 85.92%\n",
      "Train Epoch: 1 [17600/60000 (29%)]\tLoss: 2.300511,\tSparsity: 84.69%\n",
      "Train Epoch: 1 [19200/60000 (32%)]\tLoss: 2.306128,\tSparsity: 83.47%\n",
      "Train Epoch: 1 [20800/60000 (35%)]\tLoss: 2.298974,\tSparsity: 82.06%\n",
      "Train Epoch: 1 [22400/60000 (37%)]\tLoss: 2.299382,\tSparsity: 80.64%\n",
      "Train Epoch: 1 [24000/60000 (40%)]\tLoss: 2.305775,\tSparsity: 79.22%\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[0;32m<timed exec>:2\u001b[0m\n",
      "Cell \u001b[0;32mIn[314], line 21\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, optimizer, loss_fn, train_loader, test_loader, device, epoch, log_interval, test_interval, get_gradnorm)\u001b[0m\n\u001b[1;32m     18\u001b[0m     loss\u001b[39m.\u001b[39mbackward()\n\u001b[1;32m     19\u001b[0m     \u001b[39mreturn\u001b[39;00m loss\n\u001b[0;32m---> 21\u001b[0m loss \u001b[39m=\u001b[39m optimizer\u001b[39m.\u001b[39;49mstep(closure)\n\u001b[1;32m     22\u001b[0m \u001b[39m# Logging\u001b[39;00m\n\u001b[1;32m     23\u001b[0m \u001b[39mif\u001b[39;00m batch_idx \u001b[39m%\u001b[39m log_interval \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/torch/lib/python3.10/site-packages/torch/optim/optimizer.py:280\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    276\u001b[0m         \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    277\u001b[0m             \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mfunc\u001b[39m}\u001b[39;00m\u001b[39m must return None or a tuple of (new_args, new_kwargs),\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    278\u001b[0m                                \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mbut got \u001b[39m\u001b[39m{\u001b[39;00mresult\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> 280\u001b[0m out \u001b[39m=\u001b[39m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    281\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_optimizer_step_code()\n\u001b[1;32m    283\u001b[0m \u001b[39m# call optimizer step post hooks\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[316], line 68\u001b[0m, in \u001b[0;36mMaskedOptimizer.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39muse_masked_closure:\n\u001b[1;32m     67\u001b[0m     closure \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmasked_closure(closure)\n\u001b[0;32m---> 68\u001b[0m loss \u001b[39m=\u001b[39m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mstep(closure)  \u001b[39m# assuming first-order step\u001b[39;00m\n\u001b[1;32m     69\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprune_smaller_than()\n\u001b[1;32m     70\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmask_params()\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/torch/lib/python3.10/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[39mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "Cell \u001b[0;32mIn[315], line 37\u001b[0m, in \u001b[0;36mDecoupledDecay.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdecay[p] \u001b[39m=\u001b[39m group[\u001b[39m'\u001b[39m\u001b[39mlr\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m*\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdecay_rate \u001b[39m*\u001b[39m decay\n\u001b[1;32m     36\u001b[0m \u001b[39m# step\u001b[39;00m\n\u001b[0;32m---> 37\u001b[0m loss \u001b[39m=\u001b[39m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mstep(closure)\n\u001b[1;32m     38\u001b[0m \u001b[39m# apply decoupled decay\u001b[39;00m\n\u001b[1;32m     39\u001b[0m \u001b[39mfor\u001b[39;00m group \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mparam_groups:\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/torch/lib/python3.10/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[39mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "Cell \u001b[0;32mIn[317], line 25\u001b[0m, in \u001b[0;36mSAMBase.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_init_adam(closure)\n\u001b[1;32m     23\u001b[0m closure \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39menable_grad()(closure)\n\u001b[0;32m---> 25\u001b[0m loss \u001b[39m=\u001b[39m closure()\n\u001b[1;32m     26\u001b[0m grad_norm \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_grad_norm()\n\u001b[1;32m     27\u001b[0m \u001b[39mfor\u001b[39;00m group \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mparam_groups:\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/torch/lib/python3.10/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[39mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "Cell \u001b[0;32mIn[314], line 18\u001b[0m, in \u001b[0;36mtrain.<locals>.closure\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m y_pred \u001b[39m=\u001b[39m model(x)\n\u001b[1;32m     17\u001b[0m loss \u001b[39m=\u001b[39m loss_fn(y_pred, y)\n\u001b[0;32m---> 18\u001b[0m loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[1;32m     19\u001b[0m \u001b[39mreturn\u001b[39;00m loss\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/torch/lib/python3.10/site-packages/torch/_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    477\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    478\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    479\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    480\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    485\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[1;32m    486\u001b[0m     )\n\u001b[0;32m--> 487\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[1;32m    488\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[1;32m    489\u001b[0m )\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/torch/lib/python3.10/site-packages/torch/autograd/__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    195\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[1;32m    197\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    198\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    199\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 200\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    201\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[1;32m    202\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for epoch in range(1, args.epochs + 1):\n",
    "    results = train(model, optimizer, loss_fn, train_loader, test_loader, device, epoch)\n",
    "    data += results\n",
    "plt.plot(np.array(data)[:,4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 2.3026, Accuracy: 1135/10000 (11%), Sparsity: 78.88%\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(2.302644732873887, 0.0, 0.8865, 0.7887997177345153)"
      ]
     },
     "execution_count": 331,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test(model, loss_fn, test_loader, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "@torch.no_grad()\n",
    "def plot_stats(optimizer, data=None):\n",
    "    p1 = torch.cat([p.view(-1) for group in optimizer.param_groups for p in group['params']]).cpu().abs()\n",
    "    g1 = torch.cat([optimizer.state[p]['exp_avg'].view(-1) for group in optimizer.param_groups for p in group['params']]).cpu().abs()\n",
    "    g2 = torch.cat([optimizer.state[p]['exp_avg_sq'].view(-1) for group in optimizer.param_groups for p in group['params']]).cpu().abs()\n",
    "    if data is not None:\n",
    "        plt.figure()\n",
    "        plt.plot(np.array(data)[:,1])\n",
    "        plt.title(\"loss\")\n",
    "        plt.figure()\n",
    "        plt.plot(np.array(data)[:,4])\n",
    "        plt.title(\"sparsity\")\n",
    "    plt.figure()\n",
    "    plt.hist(p1[p1 > 0].log10(), bins=50)\n",
    "    plt.title(\"abs(params)\")\n",
    "    plt.figure()\n",
    "    plt.hist(g1[g1 > 0].log10(), bins=50)\n",
    "    plt.title(\"abs(exp_avg)\")\n",
    "    plt.figure()\n",
    "    plt.hist(g2[g2 > 0].log10(), bins=50)\n",
    "    plt.title(\"abs(exp_avg_sq)\")\n",
    "    return p1, g1, g2\n",
    "\n",
    "@torch.no_grad()\n",
    "def plot_stats2(optimizer, data=None, num_dims=2):\n",
    "    p1 = torch.cat([p.view(-1) for group in optimizer.param_groups for p in group['params']]).cpu().abs()\n",
    "    g1 = torch.cat([optimizer.state[p]['exp_avg'].view(-1) for group in optimizer.param_groups for p in group['params']]).cpu().abs()\n",
    "    g2 = torch.cat([optimizer.state[p]['exp_avg_sq'].view(-1) for group in optimizer.param_groups for p in group['params']]).cpu().abs()\n",
    "    if data is not None:\n",
    "        plt.figure()\n",
    "        plt.plot(np.array(data)[:,1])\n",
    "        plt.title(\"loss\")\n",
    "        plt.figure()\n",
    "        plt.plot(np.array(data)[:,4])\n",
    "        plt.title(\"sparsity\")\n",
    "    plt.figure()\n",
    "    for group in optimizer.param_groups:\n",
    "        for p in group['params']:\n",
    "            if len(p.shape) == num_dims:\n",
    "                p1 = p.cpu().abs()\n",
    "                plt.hist(p1[p1 > 0].log10(), bins=50, alpha=0.5)\n",
    "    plt.title(\"abs(params)\")\n",
    "    plt.figure()\n",
    "    for group in optimizer.param_groups:\n",
    "        for p in group['params']:\n",
    "            if len(p.shape) == num_dims:\n",
    "                g1 = optimizer.state[p][\"exp_avg\"].cpu().abs()\n",
    "                plt.hist(g1[g1 > 0].log10(), bins=50, alpha=0.5)\n",
    "    plt.title(\"abs(exp_avg)\")\n",
    "    plt.figure()\n",
    "    for group in optimizer.param_groups:\n",
    "        for p in group['params']:\n",
    "            if len(p.shape) == num_dims:\n",
    "                g2 = optimizer.state[p][\"exp_avg_sq\"].cpu().abs()\n",
    "                plt.hist(g2[g2 > 0].log10(), bins=50, alpha=0.5)\n",
    "    plt.title(\"abs(exp_avg_sq)\")\n",
    "    plt.figure()\n",
    "    for group in optimizer.param_groups:\n",
    "        for p in group['params']:\n",
    "            if len(p.shape) == num_dims:\n",
    "                pg = (p * optimizer.state[p][\"exp_avg\"]).cpu().abs()\n",
    "                plt.hist(pg[pg > 0].log10(), bins=50, alpha=0.5)\n",
    "    plt.title(\"abs(pg)\")\n",
    "    return p1, g1, g2\n",
    "\n",
    "\n",
    "_ = plot_stats2(optimizer, data)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def set_model_flat_params(model, flat_params):\n",
    "    j = 0\n",
    "    for p in model.parameters():\n",
    "        p.data.copy_(flat_params[j:j + p.numel()].reshape_as(p).data)\n",
    "        j += p.numel()\n",
    "\n",
    "@torch.no_grad()\n",
    "def get_flat_grads(optimizer):\n",
    "    return torch.cat([p.grad.view(-1) for group in optimizer.param_groups for p in group['params'] if p.grad is not None])\n",
    "\n",
    "@torch.no_grad()\n",
    "def g2_prune_smaller_than(optimizer, threshold=None):\n",
    "    # if threshold is None:\n",
    "    #     threshold = self.prune_threshold\n",
    "    flat_params = get_flat_params(optimizer)\n",
    "    flat_exp_avg_sq = torch.cat([optimizer.state[p][\"exp_avg_sq\"].view(-1)\n",
    "                                 for group in optimizer.param_groups for p in group['params'] if \"exp_avg_sq\" in optimizer.state[p]])\n",
    "    flat_params[flat_exp_avg_sq.abs() < threshold] = 0.\n",
    "    set_flat_params(optimizer, flat_params)\n",
    "    return flat_params\n",
    "    # self.update_mask()\n",
    "\n",
    "@torch.no_grad()\n",
    "def g2_prune_bigger_than(optimizer, threshold=None):\n",
    "    # if threshold is None:\n",
    "    #     threshold = self.prune_threshold\n",
    "    flat_params = get_flat_params(optimizer)\n",
    "    flat_exp_avg_sq = torch.cat([optimizer.state[p][\"exp_avg_sq\"].view(-1)\n",
    "                                 for group in optimizer.param_groups for p in group['params'] if \"exp_avg_sq\" in optimizer.state[p]])\n",
    "    flat_params[flat_exp_avg_sq.abs() > threshold] = 0.\n",
    "    set_flat_params(optimizer, flat_params)\n",
    "    return flat_params\n",
    "    # self.update_mask()\n",
    "\n",
    "@torch.no_grad()\n",
    "def plot_stats(optimizer, data=None):\n",
    "    p1 = torch.cat([p.view(-1) for group in optimizer.param_groups for p in group['params']]).cpu().abs()\n",
    "    g1 = torch.cat([optimizer.state[p]['exp_avg'].view(-1) for group in optimizer.param_groups for p in group['params']]).cpu().abs()\n",
    "    g2 = torch.cat([optimizer.state[p]['exp_avg_sq'].view(-1) for group in optimizer.param_groups for p in group['params']]).cpu().abs()\n",
    "    if data is not None:\n",
    "        plt.figure()\n",
    "        plt.plot(np.array(data)[:,4])\n",
    "        plt.title(\"sparsity\")\n",
    "    plt.figure()\n",
    "    plt.hist(p1[p1 > 0].log10(), bins=50)\n",
    "    plt.title(\"abs(params)\")\n",
    "    plt.figure()\n",
    "    plt.hist(g1[g1 > 0].log10(), bins=50)\n",
    "    plt.title(\"abs(exp_avg)\")\n",
    "    plt.figure()\n",
    "    plt.hist(g2[g2 > 0].log10(), bins=50)\n",
    "    plt.title(\"abs(exp_avg_sq)\")\n",
    "    return p1, g1, g2\n",
    "\n",
    "@torch.no_grad()\n",
    "def plot_stats2(optimizer, data=None, num_dims=2):\n",
    "    p1 = torch.cat([p.view(-1) for group in optimizer.param_groups for p in group['params']]).cpu().abs()\n",
    "    g1 = torch.cat([optimizer.state[p]['exp_avg'].view(-1) for group in optimizer.param_groups for p in group['params']]).cpu().abs()\n",
    "    g2 = torch.cat([optimizer.state[p]['exp_avg_sq'].view(-1) for group in optimizer.param_groups for p in group['params']]).cpu().abs()\n",
    "    if data is not None:\n",
    "        plt.figure()\n",
    "        plt.plot(np.array(data)[:,4])\n",
    "        plt.title(\"sparsity\")\n",
    "    plt.figure()\n",
    "    for group in optimizer.param_groups:\n",
    "        for p in group['params']:\n",
    "            if len(p.shape) == num_dims:\n",
    "                p1 = p.cpu().abs()\n",
    "                plt.hist(p1[p1 > 0].log10(), bins=50, alpha=0.5)\n",
    "    plt.title(\"abs(params)\")\n",
    "    plt.figure()\n",
    "    for group in optimizer.param_groups:\n",
    "        for p in group['params']:\n",
    "            if len(p.shape) == num_dims:\n",
    "                g1 = optimizer.state[p][\"exp_avg\"].cpu().abs()\n",
    "                plt.hist(g1[g1 > 0].log10(), bins=50, alpha=0.5)\n",
    "    plt.title(\"abs(exp_avg)\")\n",
    "    plt.figure()\n",
    "    for group in optimizer.param_groups:\n",
    "        for p in group['params']:\n",
    "            if len(p.shape) == num_dims:\n",
    "                g2 = optimizer.state[p][\"exp_avg_sq\"].cpu().abs()\n",
    "                plt.hist(g2[g2 > 0].log10(), bins=50, alpha=0.5)\n",
    "    plt.title(\"abs(exp_avg_sq)\")\n",
    "    plt.figure()\n",
    "    for group in optimizer.param_groups:\n",
    "        for p in group['params']:\n",
    "            if len(p.shape) == num_dims:\n",
    "                pg = (p * optimizer.state[p][\"exp_avg\"]).cpu().abs()\n",
    "                plt.hist(pg[pg > 0].log10(), bins=50, alpha=0.5)\n",
    "    plt.title(\"abs(pg)\")\n",
    "    return p1, g1, g2\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
